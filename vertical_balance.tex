\documentclass[12pt]{scrartcl}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{graphicx}
\usepackage{float}
\usepackage{array}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{bm}
\usepackage{verbatim}
\usepackage[english]{babel}
\usepackage{color}
\usepackage{url}
\usepackage{fancybox}
\usepackage[stable]{footmisc}
\usepackage[format=plain,labelfont=bf]{caption}
\usepackage{natbib}
\usepackage{calc}
\usepackage{textcomp}
\usepackage[pdftex,pdfborder={0 0 0}]{hyperref}
\usepackage{pdfpages}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,decorations.pathmorphing,shadows,positioning}
\usepackage{mathtools}
\renewcommand*{\familydefault}{\sfdefault}
\pgfdeclarelayer{background}
\pgfdeclarelayer{foreground}
\pgfsetlayers{background,main,foreground}

% Margins
\addtolength{\textheight}{1.0cm}
\addtolength{\oddsidemargin}{-0.5cm}
\addtolength{\evensidemargin}{-0.5cm}
\addtolength{\textwidth}{1.0cm}
\parindent=0em

% New math commands
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\Var}{Var}

% Appropriate font for \mathcal{}
\DeclareSymbolFont{cmmathcal}{OMS}{cmsy}{m}{n}
\DeclareSymbolFontAlphabet{\mathcal}{cmmathcal}

% Set subscript and superscripts positions
\everymath{
\fontdimen13\textfont2=5pt
\fontdimen14\textfont2=5pt
\fontdimen15\textfont2=5pt
\fontdimen16\textfont2=5pt
\fontdimen17\textfont2=5pt
}

% Bibliography style
\setlength{\bibsep}{1pt}

\begin{document}
\title{Vertical balance equations}
\author{Benjamin Ménétrier}
\date{Last update: \today}

\thispagestyle{empty}

\maketitle

\tableofcontents

\clearpage

\section{Block formulation}
Let $\mathbf{x} \in \mathbb{R}^n$ be the ``balanced variable'' and $\mathbf{v} \in \mathbb{R}^n$ be the ``unbalanced variable''. They are linked through a ``balance operator'' $\mathbf{K} \in \mathbb{R}^{n \times n}$:
\begin{align}
\mathbf{x} = \mathbf{K} \mathbf{v}
\end{align}
Let split $\mathbf{x}$ and $\mathbf{v}$ into $m$ blocks, and $\mathbf{K}$ into $m^2$ square blocks:
\begin{align}
\left(\begin{array}{c}
\mathbf{x}_1 \\
\vdots \\
\mathbf{x}_m
\end{array} \right) = \left(\begin{array}{ccc}
\mathbf{K}_{1,1} & \dots & \mathbf{K}_{1,m} \\
\vdots & \ddots & \vdots \\
\mathbf{K}_{m,1} & \dots & \mathbf{K}_{m,m} 
\end{array} \right) \left(\begin{array}{c}
\mathbf{v}_1 \\
\vdots \\
\mathbf{v}_m
\end{array} \right)
\end{align}
or
\begin{align}
\label{eq:full}
\mathbf{x}_i = \sum_{j=1}^m \mathbf{K}_{i,j} \mathbf{v}_j
\end{align}
where the size of each sub-vector $\mathbf{x}_i$ and $\mathbf{v}_i$ is $n_i$, with $\displaystyle \sum_{i=1}^m n_i = n$, and $\mathbf{K}_{i,j} \in \mathbb{R}^{n_i \times n_j}$.

\section{Triangular assumption}
We assume that the balance operator $\mathbf{K}$ is block-lower-triangular, with identity diagonal blocks:
\begin{align}
\mathbf{K} = \left(\begin{array}{ccccc}
\mathbf{I}_{n_1} & 0 & \dots & \dots & 0 \\
\mathbf{K}_{2,1} & \mathbf{I}_{n_2} & \ddots & & \vdots \\
\mathbf{K}_{3,1} & \mathbf{K}_{3,2} & \mathbf{I}_{n_3} & \ddots & \vdots \\
\vdots & \vdots & \ddots & \ddots & 0 \\
\mathbf{K}_{m,1} & \mathbf{K}_{m,2} & \dots & \mathbf{K}_{m,m-1} & \mathbf{I}_{n_m}
\end{array} \right) 
\end{align}
Thus, we can simplify equation \eqref{eq:full} into:
\begin{align}
\label{eq:triangle}
\mathbf{x}_i = \sum_{j=1}^{i-1} \mathbf{K}_{i,j} \mathbf{v}_j + \mathbf{v}_i
\end{align}

\section{Adjoint}
The adjoint of the balance operator is given by: 
\begin{align}
\mathbf{K}^\mathrm{T} = \left(\begin{array}{ccccc}
\mathbf{I}_{n_1} & \mathbf{K}^\mathrm{T}_{2,1} & \mathbf{K}^\mathrm{T}_{3,1} & \dots & \mathbf{K}^\mathrm{T}_{m,1} \\
0 & \mathbf{I}_{n_2} & \mathbf{K}^\mathrm{T}_{3,2} & \dots & \mathbf{K}^\mathrm{T}_{m,2} \\
\vdots & \ddots & \mathbf{I}_n & \ddots & \vdots \\
\vdots &  & \ddots & \ddots & \mathbf{K}^\mathrm{T}_{m,m-1} \\
0 & \dots & \dots & 0 & \mathbf{I}_{n_m}
\end{array} \right) 
\end{align}
so $\mathbf{v} = \mathbf{K}^\mathrm{T} \mathbf{x}$ can be expressed as:
\begin{align}
\label{eq:triangle_ad}
\mathbf{v}_i = \mathbf{x}_i + \sum_{j=i+1}^{m} \mathbf{K}^\mathrm{T}_{j,i} \mathbf{x}_j
\end{align}

\section{Partial recursive inverse}
Equation \eqref{eq:triangle} can be transformed to compute $\mathbf{v}$ knowing $\mathbf{x}$ recursively:
\begin{align}
\label{eq:partial_inverse}
\mathbf{v}_i  & = \mathbf{x}_i - \sum_{j=1}^{i-1} \mathbf{K}_{i,j} \mathbf{v}_j
\end{align}
It should be noted that the inverse of $\mathbf{K}_{i,j}$ is not required. This inverse formula is refered to as ``partial'' since the unbalanced variable $\mathbf{v}$ still appears on the right-hand side.\\
$  $\\
For the adjoint in equation \eqref{eq:triangle_ad}, the same transformation leads to:
\begin{align}
\label{eq:partial_inverse_ad}
\mathbf{x}_i = \mathbf{v}_i - \sum_{j=i+1}^{m} \mathbf{K}^\mathrm{T}_{j,i} \mathbf{x}_j
\end{align}
and must be applied recursively in reverse order ($i$ from $m$ to 1).

\section{Full recursive inverse}
We assume that for $1 \le j \le i$, there are matrices $\mathbf{A}_{i, j} \in \mathbb{R}^{n_i \times n_j}$ such that $\mathbf{v}_i$ can be expressed only as a function of $\mathbf{x}$:
\begin{align}
\label{eq:full_inverse}
\mathbf{v}_i = \sum_{j=1}^i \mathbf{A}_{i,j} \mathbf{x}_j
\end{align}
Plugging this expression into the partial recursive inverse formula \eqref{eq:partial_inverse}, we get:
\begin{align}
\mathbf{v}_i & = \mathbf{x}_i - \sum_{j=1}^{i-1} \mathbf{K}_{i,j} \mathbf{v}_j \nonumber \\
& = \mathbf{x}_i - \sum_{j=1}^{i-1} \sum_{k=1}^j \mathbf{K}_{i,j} \mathbf{A}_{j,k} \mathbf{x}_k \nonumber \\
& = \mathbf{x}_i - \sum_{k=1}^{i-1} \sum_{j=k}^{i-1} \mathbf{K}_{i,j} \mathbf{A}_{j,k} \mathbf{x}_k \nonumber \\
& = \mathbf{x}_i - \sum_{j=1}^{i-1} \sum_{k=j}^{i-1} \mathbf{K}_{i,k} \mathbf{A}_{k,j} \mathbf{x}_j \nonumber \\
& =\sum_{j=1}^{i-1} \left(-\sum_{k=j}^{i-1} \mathbf{K}_{i,k} \mathbf{A}_{k,j}\right) \mathbf{x}_j + \mathbf{x}_i
\end{align}
which is consistent with \eqref{eq:full_inverse} if:
\begin{subequations}
\begin{align}
\mathbf{A}_{i,j} & = - \sum_{k=j}^{i-1} \mathbf{K}_{i,k} \mathbf{A}_{k,j} \text{ (for }j < i\text{)}\\
\mathbf{A}_{i,i} & = \mathbf{I}_{n_i}
\end{align}
\end{subequations}
For convenience, we also define the matrices $\mathbf{A}_{i,j}$ for $j > i$ to make the full matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$:
\begin{align}
\mathbf{A}_{i,j} = \mathbf{A}_{j,i}^\mathrm{T}
\end{align}

\section{Sequential estimation}
Since $\mathbf{v}$ is an ``unbalanced variable'', the component $\mathbf{v}_i$ should be uncorrelated with all the other components:
\begin{align}
\label{eq:stat_prop}
\text{if } j \ne i \text{, }\Cov\left(\mathbf{v}_i, \mathbf{v}_j\right) = 0
\end{align}

Using the partial recursive inverse \eqref{eq:partial_inverse} for $j < i$, we get:
\begin{align}
& \Cov\left(\mathbf{v}_i, \mathbf{v}_j\right) = 0 \nonumber \\
\Leftrightarrow \ & \Cov\left(\mathbf{x}_i - \sum_{k=1}^{i-1} \mathbf{K}_{i,k} \mathbf{v}_k, \mathbf{v}_j\right) = 0 \nonumber \\
\Leftrightarrow \ & \Cov\left(\mathbf{x}_i, \mathbf{v}_j\right) - \sum_{k=1}^{i-1} \mathbf{K}_{i,k} \Cov\left(\mathbf{v}_k, \mathbf{v}_j\right) = 0
\end{align}
Using the statistical property \eqref{eq:stat_prop} of $\mathbf{v}$, the term for which $k=j$ is the only one remaining:
\begin{align}
\label{eq:partial_K}
& \Cov\left(\mathbf{x}_i, \mathbf{v}_j\right) - \mathbf{K}_{i,j} \Cov\left(\mathbf{v}_j, \mathbf{v}_j\right) = 0 \nonumber \\
& \mathbf{K}_{i,j} = \Cov\left(\mathbf{x}_i, \mathbf{v}_j\right) \Cov\left(\mathbf{v}_j, \mathbf{v}_j\right)^{-1}
\end{align}

Using the full recursive inverse \eqref{eq:full_inverse}, we get for $j < i$:
\begin{subequations}
\begin{align}
\Cov\left(\mathbf{x}_i, \mathbf{v}_j\right) & = \Cov\left(\mathbf{x}_i, \sum_{k=1}^j \mathbf{A}_{j,k} \mathbf{x}_k\right) \nonumber \\
 & = \sum_{k=1}^j\Cov\left(\mathbf{x}_i, \mathbf{x}_k\right) \mathbf{A}_{j,k}^\mathrm{T} \\
\Cov\left(\mathbf{v}_j, \mathbf{v}_j\right) & = \Cov\left(\sum_{k=1}^j \mathbf{A}_{j,k} \mathbf{x}_k, \sum_{l=1}^j \mathbf{A}_{j,l} \mathbf{x}_l\right) \nonumber \\
 & = \sum_{k=1}^j \mathbf{A}_{j,k} \sum_{l=1}^j \Cov\left(\mathbf{x}_k, \mathbf{x}_l\right) \mathbf{A}_{j,l}^\mathrm{T}
\end{align}
\end{subequations}
which allows a computation of the balance operator $\mathbf{K}_{i,j}$ using expression \eqref{eq:partial_K}.

\section{Aggregated formulation}
For practical reasons explained later in section \ref{sec:practical}, we define aggregated quantities:
\begin{itemize}
\item the aggregated size $\displaystyle \overbar{n}_i = \sum_{j=1}^{i-1} n_j$
\item the aggregated vectors $\overbar{\mathbf{x}}_i$ and $\overbar{\mathbf{v}}_i \in \mathbb{R}^{\overbar{n}_i}$:
\begin{align}
\overbar{\mathbf{x}}_i = \left(\begin{array}{c}
\mathbf{x}_1 \\
\vdots \\
\mathbf{x}_{i-1}
\end{array}
\right) \ \text{ and } \ \overbar{\mathbf{v}}_i = \left(\begin{array}{c}
\mathbf{v}_1 \\
\vdots \\
\mathbf{v}_{i-1}
\end{array}
\right)
\end{align}
\item the aggregated operator $\overbar{\mathbf{K}}_i \in \mathbb{R}^{n_i \times \overbar{n}_i}$:
\begin{align}
\overbar{\mathbf{K}}_i = \left(\mathbf{K}_{i,1} \ \dots \ \mathbf{K}_{i,i-1}\right)
\end{align}
\item the aggregated matrix $\overbar{\mathbf{A}}_i \in \mathbb{R}^{n_i \times \overbar{n}_i}$:
\begin{align}
\overbar{\mathbf{A}}_i = \left(\mathbf{A}_{i,1} \ \dots \ \mathbf{A}_{i,i-1}\right)
\end{align}
\end{itemize}
Thus, equation \eqref{eq:triangle} can be rewritten as:
\begin{align}
\label{eq:triangle_aggregated}
\mathbf{x}_i = \overbar{\mathbf{K}}_i \overbar{\mathbf{v}}_i + \mathbf{v}_i
\end{align}
The partial inverse formulation \eqref{eq:partial_inverse} can be rewritten as:
\begin{align}
\label{eq:partial_inverse_aggregated}
\mathbf{v}_i  & = \mathbf{x}_i - \overbar{\mathbf{K}}_i \overbar{\mathbf{v}}_i
\end{align}
and the full inverse formulation \eqref{eq:full_inverse} can be rewritten as:
\begin{align}
\label{eq:full_inverse_aggregated}
\mathbf{v}_i = \overbar{\mathbf{A}}_i \overbar{\mathbf{x}}_i + \mathbf{x}_i
\end{align}
with $\mathbf{A}_{i,j}$ computed as:
\begin{align}
\mathbf{A}_{i,j} & = - \overbar{\mathbf{K}}_i \overbar{\mathbf{A}}_j^\mathrm{T}
\end{align}
Finally, we define the matrix $\overbar{\overbar{\mathbf{A}}}_i \in \mathbb{R}^{\overbar{n}_i \times \overbar{n}_i}$:
\begin{align}
\overbar{\overbar{\mathbf{A}}}_i = \left(\begin{array}{cccc}
\mathbf{I} & 0 & \dots & 0 \\
\mathbf{A}_{2,1} & \mathbf{I} & \ddots & \vdots \\
\vdots & \ddots & \ddots & 0 \\
\mathbf{A_{i-1,1}} & \dots & \mathbf{A_{i-1,i-2}} & \mathbf{I}
\end{array}\right)
\end{align}
to get the aggregated form:
\begin{align}
\label{eq:aggregated}
\overbar{\mathbf{v}}_i = \overbar{\overbar{\mathbf{A}}}_i \overbar{\mathbf{x}}_i
\end{align}
$  $\\
The assumption \eqref{eq:stat_prop} can be rewritten as:
\begin{align}
\label{eq:stat_prop_aggregated}
\Cov\left(\mathbf{v}_i, \overbar{\mathbf{v}}_i\right) = 0
\end{align}
leading to the equivalent of equation \eqref{eq:partial_K}:
\begin{align}
\label{eq:partial_K_aggregated}
\overbar{\mathbf{K}}_i = \Cov\left(\mathbf{x}_i, \overbar{\mathbf{v}}_i\right) \Cov\left(\overbar{\mathbf{v}}_i, \overbar{\mathbf{v}}_i\right)^{-1}
\end{align}
Expanding eqation \eqref{eq:stat_prop_aggregated} with the partial recursive inverse \eqref{eq:partial_inverse_aggregated} we get:
\begin{align}
\label{eq:partial_K_aggregated}
& \Cov\left(\mathbf{v}_i, \overbar{\mathbf{v}}_i\right) = 0 \nonumber \\
\Leftrightarrow \ & \Cov\left(\mathbf{x}_i - \overbar{\mathbf{K}}_i \overbar{\mathbf{v}}_i, \overbar{\mathbf{v}}_i\right) = 0 \nonumber \\
\Leftrightarrow \ & \Cov\left(\mathbf{x}_i, \overbar{\mathbf{v}}_i\right) - \overbar{\mathbf{K}}_i \Cov\left(\overbar{\mathbf{v}}_i, \overbar{\mathbf{v}}_i\right) = 0 \nonumber \\
\Leftrightarrow \ & \overbar{\mathbf{K}}_i = \Cov\left(\mathbf{x}_i, \overbar{\mathbf{v}}_i\right) \Cov\left(\overbar{\mathbf{v}}_i, \overbar{\mathbf{v}}_i\right)^{-1}
\end{align}
Using the aggregated full recursive inverse \eqref{eq:full_inverse_aggregated} and the aggregated form \eqref{eq:aggregated}, we get:
\begin{subequations}
\begin{align}
\Cov\left(\mathbf{x}_i, \overbar{\mathbf{v}}_i\right) & = \Cov\left(\mathbf{x}_i, \overbar{\overbar{\mathbf{A}}}_i \overbar{\mathbf{x}}_i\right) \nonumber \\
 & = \Cov\left(\mathbf{x}_i, \overbar{\mathbf{x}}_i\right) \overbar{\overbar{\mathbf{A}}}_i^\mathrm{T} \\
\Cov\left(\overbar{\mathbf{v}}_i, \overbar{\mathbf{v}}_i\right) & = \Cov\left(\overbar{\overbar{\mathbf{A}}}_i \overbar{\mathbf{x}}_i, \overbar{\overbar{\mathbf{A}}}_i \overbar{\mathbf{x}}_i\right) \nonumber \\
 & = \overbar{\overbar{\mathbf{A}}}_i \Cov\left(\overbar{\mathbf{x}}_i,  \overbar{\mathbf{x}}_i\right) \overbar{\overbar{\mathbf{A}}}_i^\mathrm{T}
\end{align}
\end{subequations}
which allows a computation of the balance operator $\overbar{\mathbf{K}}_i$ using expression \eqref{eq:partial_K_aggregated}.

\section{Practical computations}
\label{sec:practical}
In practice, covariances are sampled from an ensemble. Let $\left\{\widetilde{\mathbf{x}}^1, \dots, \widetilde{\mathbf{x}}^N\right\}$ be a set of $N$ vectors that samples the distribution of $\mathbf{x}$ and $\left\{\delta \widetilde{\mathbf{x}}^1, \dots, \delta \widetilde{\mathbf{x}}^N\right\}$ their centered counterparts:
\begin{align}
\delta \widetilde{\mathbf{x}}^p = \widetilde{\mathbf{x}}^p - \langle \widetilde{\mathbf{x}} \rangle
\end{align}
where $\langle \cdot \rangle$ denotes the ensemble mean:
\begin{align}
\label{eq:exp_estim}
\langle \widetilde{\mathbf{x}} \rangle = \frac{1}{N} \sum_{p=1}^N \widetilde{\mathbf{x}}^p
\end{align}
The covariance $\Cov \left(\mathbf{x}_i, \mathbf{x}_j\right)$ can be estimated from these perturbations by:
\begin{align}
\label{eq:cov_estim}
\widetilde{\Cov}\left(\mathbf{x}_i, \mathbf{x}_j\right) & = \frac{1}{N-1} \sum_{p=1}^N \delta \widetilde{\mathbf{x}}^p_i \left(\delta \widetilde{\mathbf{x}}^p_j\right)^\mathrm{T}
\end{align}
Because of the limited ensemble size $N$, it can be relevant to filter the covariance matrix with a linear operator $\mathcal{F}$:
\begin{align}
\label{eq:cov_filter}
\widehat{\Cov}\left(\mathbf{x}_i, \mathbf{x}_j\right) = \mathcal{F} \left[\widetilde{\Cov}\left(\mathbf{x}_i, \mathbf{x}_j\right)\right]
\end{align}
$ $\\
Since regressions are estimated with approximate covariances, the non-aggregated and the aggregated formulations give different results in practice. Indeed, the aggregated formulation will actually compute the cross-covariances between unbalanced variables (non-diagonal blocks) in $\widehat{\Cov}\left(\overbar{\mathbf{v}}_i, \overbar{\mathbf{v}}_i\right)$, whereas the non-aggregated formulation assumes that $\widehat{\Cov}\left(\overbar{\mathbf{v}}_i, \overbar{\mathbf{v}}_i\right)$ is block-diagonal. Hereafter, we describe calculations for the aggregated formulation. The non-aggregated formulation is simply obtained by forcing non-diagonal block to zero in $\widehat{\Cov}\left(\overbar{\mathbf{v}}_i, \overbar{\mathbf{v}}_i\right)$.\\
$  $\\
The computation of the balance operator components using the partial recursive inverse equation \eqref{eq:partial_inverse_aggregated} is detailed in algorithm \ref{algo:partial}. Its counterpart using the full recursive inverse equation \eqref{eq:full_inverse_aggregated} is detailed in algorithm \ref{algo:full}. It should be noted that algorithm \ref{algo:partial} requires storing the ensemble twice, while algorithm \ref{algo:full} requires storing more matrices.

\section{Time-averaging}

Let's assume that we have several occurences of an ensemble for different dates/times. A sliding average, possibly with different weights, sounds like a reasonable idea to increase the sampling size and reduce the sampling noise. However, this should be done carefully:
\begin{itemize}
\item Since regressions $\mathbf{K}_{i,j}$ are ratios of covariances, it is not relevant to average them overtime. Moreover, as shown in section \ref{sec:noise}, regressions can be very sensitive to sampling noise and their estimation very unstable.
\item Covariances $\widehat{\Cov}\left(\mathbf{x}_i, \overbar{\mathbf{v}}_i\right)$ and $\widehat{\Cov}\left(\overbar{\mathbf{v}}_i, \overbar{\mathbf{v}}_i\right)$ involve unbalanced covariances, that are comp$\mathbf{K}_{i,j}$ dependent on the sequential computation of regressions, so they should be not be averaged neither.
\item Only the covariances $\widetilde{\Cov}\left(\mathbf{x}_i, \mathbf{x}_j\right)$ are actual covariances of balanced variables, that can be averaged safely. Thus, only the full recursive inverse formula should be used to compute time-averaged regressions, as shown in algorithm \ref{algo:full_averaged}.
\end{itemize}

\section{Regression estimation error}
\label{sec:noise}

To characterize theoretically the error arising when estimating a regression operator, we define the simplest possible framework. The background error covariance matrix $\mathbf{B} \in \mathbb{R}^{2 \times 2}$ is defined as:
\begin{align}
\mathbf{B} = \mathbf{K} \mathbf{C}_u \mathbf{K}^\mathrm{T}
\end{align}
where $\mathbf{K} \in \mathbb{R}^{2 \times 2}$ is defined by a scalar regression $r$:
\begin{align}
\mathbf{K} = \left( \begin{array}{cc}
1 & 0 \\
r & 1
\end{array} \right)
\end{align}
and $\mathbf{C}_u \in \mathbb{R}^{2 \times 2}$ is the diagonal matrix of variances:
\begin{align}
\mathbf{C}_u = \left( \begin{array}{cc}
\sigma_1^2 & 0 \\
0 & \sigma_2^2
\end{array} \right)
\end{align}
Thus:
\begin{align}
\mathbf{B} = \left( \begin{array}{cc}
\sigma_1^2 & r \sigma_1^2\\
r \sigma_1^2 & r^2 \sigma_1^2 + \sigma_2^2
\end{array} \right)
\end{align}
and the correlation matrix is given by:
\begin{align}
\mathbf{C} = \left( \begin{array}{cc}
1 & c\\
c & 1
\end{array} \right)
\end{align}
with:
\begin{align}
\label{eq:correlation}
c & = \frac{r \sigma_1}{\sqrt{r^2 \sigma_1^2 + \sigma_2^2}} \nonumber \\
& = \left(1 + \left(\displaystyle \frac{\sigma_2}{r \sigma_1}\right)^2\right)^{-1/2}
\end{align}
A possible square-root of $\mathbf{B}$ is $\mathbf{U} \in \mathbb{R}^{2 \times 2}$:
\begin{align}
\mathbf{U} = \left( \begin{array}{cc}
\sigma_1 & 0 \\
r \sigma_1 & \sigma_2
\end{array} \right)
\end{align}
We assume that an ensemble of $N$ perturbations $\boldsymbol{\zeta}$ is generated by randomization:
\begin{align}
\boldsymbol{\zeta} & = \mathbf{U} \boldsymbol{\eta} \nonumber \\
 & = \left( \begin{array}{c}
\sigma_1 \eta_1 \\
r \sigma_1 \eta_1 + \sigma_2 \eta_2
\end{array} \right)
\end{align}
where $\boldsymbol{\eta} \in \mathbb{R}^2$ is a Gaussian deviate of zero mean and unit variance: $\boldsymbol{\eta} \sim \mathcal{N}\left(0, \mathbf{I}_2\right)$.\\
$  $\\
The regression coefficient $r$ is estimated as:
\begin{align}
\widetilde{r} & = \frac{\widetilde{\Cov}\left(\zeta_2, \zeta_1\right)}{\widetilde{\Var}\left(\zeta_1\right)} \nonumber \\
& = \frac{r \sigma_1^2 \widetilde{\Var}\left(\eta_1\right) + \sigma_1 \sigma_2 \widetilde{\Cov}\left(\eta_2, \eta_1\right)}{\sigma_1^2 \widetilde{\Var}\left(\eta_1\right)} \nonumber \\
& = r + \frac{\sigma_2 \widetilde{\Cov}\left(\eta_2, \eta_1\right)}{\sigma_1 \widetilde{\Var}\left(\eta_1\right)}
\end{align}
Thus, the estimation error on $r$ is:
\begin{align}
\widetilde{r} - r = \frac{\sigma_2 \widetilde{\Cov}\left(\eta_2, \eta_1\right)}{\sigma_1 \widetilde{\Var}\left(\eta_1\right)}
\end{align}
Due to the limited sample size, the estimations are noisy:
\begin{subequations}
\begin{align}
\widetilde{\Cov}\left(\eta_2, \eta_1\right) & = \Cov\left(\eta_2, \eta_1\right) + \varepsilon_{21} \\
\widetilde{\Var}\left(\eta_1\right) & = \Var\left(\eta_1\right) + \varepsilon_1
\end{align}
\end{subequations}
where the definition of $\boldsymbol{\eta}$ yields:
\begin{subequations}
\begin{align}
\Cov\left(\eta_2, \eta_1\right) = 0 \\
\Var\left(\eta_1\right) = 1
\end{align}
\end{subequations}
and the covariance estimation theory gives:
\begin{subequations}
\begin{align}
\mathbb{E}\left[\varepsilon_{21}\right] & = 0 \\
\Var\left(\varepsilon_{21}^2\right) & = \frac{1}{N-1}
\end{align}
\end{subequations}
and 
\begin{subequations}
\begin{align}
\mathbb{E}\left[\varepsilon_1\right] & = 0 \\
\Var\left(\varepsilon_1^2\right) & = \frac{2}{N-1}
\end{align}
\end{subequations}
If $N$ is large enough, $\varepsilon_1 \ll 1$ so:
\begin{align}
\frac{1}{\widetilde{\Var}\left(\eta_1\right)} & = \frac{1}{1 + \varepsilon_1} \simeq 1 - \varepsilon_1
\end{align}
and $\varepsilon_{21} \ll 1$ also, leading to a first order approximation:
\begin{align}
\widetilde{r} - r & \simeq \frac{\sigma_2}{\sigma_1} \varepsilon_{21} \left(1 - \varepsilon_1\right) \simeq \frac{\sigma_2}{\sigma_1} \varepsilon_{21}
\end{align}
Thus, at first order, the error $\widetilde{r} - r$ as the following properties:
\begin{subequations}
\begin{align}
\mathbb{E}\left[\widetilde{r} - r\right] & \simeq 0 \\
\Var\left(\widetilde{r} - r\right) & \simeq \frac{\sigma_2^2}{\sigma_1^2 (N-1)}
\end{align}
\end{subequations}
We can conclude that:
\begin{itemize}
\item As expected, the accuracy of the regression estimation $\widetilde{r}$ increases when the ensemble size $N$ increases.
\item For a finite ensemble size $N$, the relative error of the regression estimation:
\begin{align}
\frac{\sqrt{\Var\left(\widetilde{r} - r\right)}}{r} \simeq \frac{\sigma_2}{r \sigma_1 (N-1)}
\end{align}
can be arbitrary large if on the ratio $\sigma_2 / (r\sigma_1)$ is significantly larger than $N-1$. However in this case, equation \eqref{eq:correlation} shows that the correlation coefficient $c$ is close to zero, which means that even a large relative error on $\widetilde{r}$ should not have a strong impact.
\end{itemize}

\clearpage

\begin{algorithm}[!ht]
\caption{Recursive computation of the balance operator components using the partial recursive inverse formula \label{algo:partial}}
\begin{algorithmic}
\STATE Copy the ensemble perturbations:
\FOR{$1 \le p \le N$}
\STATE $\delta \widetilde{\mathbf{v}}^p = \delta \widetilde{\mathbf{x}}^p$
\ENDFOR
\FOR{$1 < i \le m$}
\FOR{$1 \le j < i$}
\STATE Estimate the covariance between $\mathbf{x}_i$ and $\mathbf{v}_j$:
\STATE $\displaystyle \qquad \widetilde{\Cov}\left(\mathbf{x}_i, \mathbf{v}_j\right) = \frac{1}{N-1} \sum_{p=1}^N \delta \widetilde{\mathbf{x}}^p_i \left(\delta \widetilde{\mathbf{v}}^p_j\right)^\mathrm{T}$
\STATE Filter the covariance between $\mathbf{x}_i$ and $\mathbf{v}_j$:
\STATE $\displaystyle \qquad \widehat{\Cov}\left(\mathbf{x}_i, \mathbf{v}_j\right) = \mathcal{F} \left[\widetilde{\Cov}\left(\mathbf{x}_i, \mathbf{v}_j\right)\right]$
\ENDFOR
\STATE Concatenate matrices to build $\widehat{\Cov}\left(\mathbf{x}_i, \overbar{\mathbf{v}}_i\right)$ and $\widehat{\Cov}\left(\overbar{\mathbf{v}}_i, \overbar{\mathbf{v}}_i\right)$
\STATE Inverse $\widehat{\Cov}\left(\overbar{\mathbf{v}}_i, \overbar{\mathbf{v}}_i\right)$ to get $\widehat{\Cov}\left(\overbar{\mathbf{v}}_i, \overbar{\mathbf{v}}_i\right)^{-1}$
\STATE Compute $\overbar{\mathbf{K}}_i$:
\STATE $\displaystyle \qquad \overbar{\mathbf{K}}_i = \widehat{\Cov}\left(\mathbf{x}_i, \overbar{\mathbf{v}}_i\right) \widehat{\Cov}\left(\overbar{\mathbf{v}}_i, \overbar{\mathbf{v}}_i\right)^{-1}$
\STATE Update the unbalanced ensemble perturbations:
\FOR{$1 \le p \le N$}
\FOR{$1 \le j < i$}
\STATE $\displaystyle \qquad \delta \widetilde{\mathbf{v}}^p_i \leftarrow \delta \widetilde{\mathbf{v}}^p_i - \mathbf{K}_{i,j} \delta \widetilde{\mathbf{v}}^p_j$
\ENDFOR
\ENDFOR
\IF{$i < m$}
\FOR{$1 \le j < i$}
\STATE Estimate the covariance of $\mathbf{v}_i$ with $\mathbf{v}_j$:
\STATE $\displaystyle \qquad \widetilde{\Cov}\left(\mathbf{v}_i, \mathbf{v}_j\right) = \frac{1}{N-1} \sum_{p=1}^N \delta \widetilde{\mathbf{v}}^p_i \left(\delta \widetilde{\mathbf{v}}^p_j\right)^\mathrm{T}$
\STATE Filter the covariance of $\mathbf{v}_i$ with $\mathbf{v}_j$:
\STATE $\displaystyle \qquad \widehat{\Cov}\left(\mathbf{v}_i, \mathbf{v}_j\right) = \mathcal{F} \left[\widetilde{\Cov}\left(\mathbf{v}_i, \mathbf{v}_j\right)\right]$
\ENDFOR
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
\caption{Recursive computation of the balance operator components using the full recursive inverse formula \label{algo:full}}
\begin{algorithmic}
\FOR{$1 \le i \le m$}
\FOR{$1 \le j \le i$}
\STATE Estimate the covariance between $\mathbf{x}_i$ and $\mathbf{x}_j$:
\STATE $\displaystyle \qquad \widetilde{\Cov}\left(\mathbf{x}_i, \mathbf{x}_j\right) = \frac{1}{N-1} \sum_{p=1}^N \delta \widetilde{\mathbf{x}}^p_i \left(\delta \widetilde{\mathbf{x}}^p_j\right)^\mathrm{T}$
\ENDFOR
\ENDFOR
\FOR{$1 < i \le m$}
\FOR{$1 \le j < i$}
\STATE Compute the covariance between $\mathbf{x}_i$ and $\mathbf{v}_j$:
\STATE $\displaystyle \qquad \widetilde{\Cov}\left(\mathbf{x}_i, \mathbf{v}_j\right) = \sum_{k=1}^{j-1} \widetilde{\Cov} \left(\mathbf{x}_i, \mathbf{x}_k\right) \mathbf{A}_{j,k}^\mathrm{T} + \widetilde{\Cov} \left(\mathbf{x}_i, \mathbf{x}_j\right)$
\STATE Filter the covariance between $\mathbf{x}_i$ and $\mathbf{v}_j$:
\STATE $\displaystyle \qquad \widehat{\Cov}\left(\mathbf{x}_i, \mathbf{v}_j\right) = \mathcal{F} \left[\widetilde{\Cov}\left(\mathbf{x}_i, \mathbf{v}_j\right)\right]$
\ENDFOR
\STATE Concatenate matrices to build $\widehat{\Cov}\left(\mathbf{x}_i, \overbar{\mathbf{v}}_i\right)$ and $\widehat{\Cov}\left(\overbar{\mathbf{v}}_i, \overbar{\mathbf{v}}_i\right)$
\STATE Inverse $\widehat{\Cov}\left(\overbar{\mathbf{v}}_i, \overbar{\mathbf{v}}_i\right)$ to get $\widehat{\Cov}\left(\overbar{\mathbf{v}}_i, \overbar{\mathbf{v}}_i\right)^{-1}$
\STATE Compute $\overbar{\mathbf{K}}_i$:
\STATE $\displaystyle \qquad \overbar{\mathbf{K}}_i = \widehat{\Cov}\left(\mathbf{x}_i, \overbar{\mathbf{v}}_i\right) \widehat{\Cov}\left(\overbar{\mathbf{v}}_i, \overbar{\mathbf{v}}_i\right)^{-1}$
\FOR{$1 \le j < i$}
\STATE Compute the matrix $\mathbf{A}_{i,j}$:
\STATE $\displaystyle \qquad \mathbf{A}_{i,j} = - \sum_{k=j}^{i-1} \mathbf{K}_{i,k} \mathbf{A}_{k,j}$
\ENDFOR
\IF{$i < m$}
\FOR{$1 \le j < i$}
\STATE Compute temporary matrices $\mathbf{S}_{k,j}$ :
\FOR{$1 \le k \le i$}
\STATE $\displaystyle \mathbf{S}_{k,j} = \sum_{l=1}^{k} 
 \widetilde{\Cov}\left(\mathbf{x}_k, \mathbf{x}_l\right) \mathbf{A}_{j,l}^\mathrm{T} + \sum_{l=k+1}^{j-1} 
 \widetilde{\Cov}\left(\mathbf{x}_l, \mathbf{x}_k\right)^\mathrm{T} \mathbf{A}_{j,l}^\mathrm{T} + \widetilde{\Cov}\left(\mathbf{x}_k, \mathbf{x}_j\right)$
\ENDFOR
\STATE Compute the covariance between $\mathbf{v}_i$ and $\mathbf{v}_j$:
\STATE $\displaystyle \qquad \widetilde{\Cov}\left(\mathbf{v}_i, \mathbf{v}_j\right) = \sum_{k=1}^{i-1} \mathbf{A}_{i,k} \mathbf{S}_{k,j} + \mathbf{S}_{i,j}$
\STATE Filter the covariance between $\mathbf{v}_i$ and $\mathbf{v}_j$:
\STATE $\displaystyle \qquad \widehat{\Cov}\left(\mathbf{v}_i, \mathbf{v}_j\right) = \mathcal{F} \left[\widetilde{\Cov}\left(\mathbf{v}_i, \mathbf{v}_j\right)\right]$
\ENDFOR
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
\caption{Recursive computation of the balance operator components using the full recursive inverse formula, with an extra covariance averaging step \label{algo:full_averaged}}
\begin{algorithmic}
\FOR{$1 \le i \le m$}
\FOR{$1 \le j \le i$}
\STATE Estimate the covariance between $\mathbf{x}_i$ and $\mathbf{x}_j$:
\STATE $\displaystyle \qquad \widetilde{\Cov}\left(\mathbf{x}_i, \mathbf{x}_j\right) = \frac{1}{N-1} \sum_{p=1}^N \delta \widetilde{\mathbf{x}}^p_i \left(\delta \widetilde{\mathbf{x}}^p_j\right)^\mathrm{T}$
\STATE Read old covariance estimate $\widetilde{\Cov}_\textrm{old}\left(\mathbf{x}_i, \mathbf{x}_j\right)$ and average with the current covariance estimate:
\STATE $\displaystyle \qquad \widetilde{\Cov}\left(\mathbf{x}_i, \mathbf{x}_j\right) \leftarrow \alpha \widetilde{\Cov}\left(\mathbf{x}_i, \mathbf{x}_j\right) + (1-\alpha) \widetilde{\Cov}_\textrm{old}\left(\mathbf{x}_i, \mathbf{x}_j\right)$
\ENDFOR
\ENDFOR
\STATE As algorithm \ref{algo:full} from this point onwards.
\end{algorithmic}
\end{algorithm}

\end{document}
