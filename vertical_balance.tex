\documentclass[12pt]{scrartcl}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{graphicx}
\usepackage{float}
\usepackage{array}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{bm}
\usepackage{verbatim}
\usepackage[english]{babel}
\usepackage{color}
\usepackage{url}
\usepackage{fancybox}
\usepackage[stable]{footmisc}
\usepackage[format=plain,labelfont=bf]{caption}
\usepackage{natbib}
\usepackage{calc}
\usepackage{textcomp}
\usepackage[pdftex,pdfborder={0 0 0}]{hyperref}
\usepackage{pdfpages}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,decorations.pathmorphing,shadows,positioning}
\usepackage{mathtools}
\renewcommand*{\familydefault}{\sfdefault}
\pgfdeclarelayer{background}
\pgfdeclarelayer{foreground}
\pgfsetlayers{background,main,foreground}

% Margins
\addtolength{\textheight}{1.0cm}
\addtolength{\oddsidemargin}{-0.5cm}
\addtolength{\evensidemargin}{-0.5cm}
\addtolength{\textwidth}{1.0cm}
\parindent=0em

% New math commands
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\Var}{Var}

% Appropriate font for \mathcal{}
\DeclareSymbolFont{cmmathcal}{OMS}{cmsy}{m}{n}
\DeclareSymbolFontAlphabet{\mathcal}{cmmathcal}

% Set subscript and superscripts positions
\everymath{
\fontdimen13\textfont2=5pt
\fontdimen14\textfont2=5pt
\fontdimen15\textfont2=5pt
\fontdimen16\textfont2=5pt
\fontdimen17\textfont2=5pt
}

% Bibliography style
\setlength{\bibsep}{1pt}

\begin{document}
\title{Vertical balance equations}
\author{Benjamin Ménétrier}
\date{Last update: \today}

\thispagestyle{empty}

\maketitle

\tableofcontents

\clearpage

\section{Block formulation}
Let $\mathbf{x} \in \mathbb{R}^n$ be the ``balanced variable'' and $\mathbf{v} \in \mathbb{R}^n$ be the ``unbalanced variable''. They are linked through a ``balance operator'' $\mathbf{K} \in \mathbb{R}^{n \times n}$:
\begin{align}
\mathbf{x} = \mathbf{K} \mathbf{v}
\end{align}
Let split $\mathbf{x}$ and $\mathbf{v}$ into $m$ blocks, and $\mathbf{K}$ into $m^2$ square blocks:
\begin{align}
\left(\begin{array}{c}
\mathbf{x}_1 \\
\vdots \\
\mathbf{x}_m
\end{array} \right) = \left(\begin{array}{ccc}
\mathbf{K}_{1,1} & \dots & \mathbf{K}_{1,m} \\
\vdots & \ddots & \vdots \\
\mathbf{K}_{m,1} & \dots & \mathbf{K}_{m,m} 
\end{array} \right) \left(\begin{array}{c}
\mathbf{v}_1 \\
\vdots \\
\mathbf{v}_m
\end{array} \right)
\end{align}
or
\begin{align}
\label{eq:full}
\mathbf{x}_i = \sum_{j=1}^m \mathbf{K}_{i,j} \mathbf{v}_j
\end{align}
where the size of each sub-vector $\mathbf{x}_i$ and $\mathbf{v}_i$ is $n_i$, with $\displaystyle \sum_{i=1}^m n_i = n$, and $\mathbf{K}_{i,j} \in \mathbb{R}^{n_i \times n_j}$.

\section{Triangular assumption}
We assume that the balance operator $\mathbf{K}$ is block-lower-triangular, with identity diagonal blocks:
\begin{align}
\mathbf{K} = \left(\begin{array}{ccccc}
\mathbf{I}_{n_1} & 0 & \dots & \dots & 0 \\
\mathbf{K}_{2,1} & \mathbf{I}_{n_2} & \ddots & & \vdots \\
\mathbf{K}_{3,1} & \mathbf{K}_{3,2} & \mathbf{I}_{n_3} & \ddots & \vdots \\
\vdots & \vdots & \ddots & \ddots & 0 \\
\mathbf{K}_{m,1} & \mathbf{K}_{m,2} & \dots & \mathbf{K}_{m,m-1} & \mathbf{I}_{n_m}
\end{array} \right) 
\end{align}
Thus, we can simplify equation \eqref{eq:full} into:
\begin{align}
\label{eq:triangle}
\mathbf{x}_i = \sum_{j=1}^{i-1} \mathbf{K}_{i,j} \mathbf{v}_j + \mathbf{v}_i
\end{align}

\section{Adjoint}
The adjoint of the balance operator is given by: 
\begin{align}
\mathbf{K}^\mathrm{T} = \left(\begin{array}{ccccc}
\mathbf{I}_{n_1} & \mathbf{K}^\mathrm{T}_{2,1} & \mathbf{K}^\mathrm{T}_{3,1} & \dots & \mathbf{K}^\mathrm{T}_{m,1} \\
0 & \mathbf{I}_{n_2} & \mathbf{K}^\mathrm{T}_{3,2} & \dots & \mathbf{K}^\mathrm{T}_{m,2} \\
\vdots & \ddots & \mathbf{I}_n & \ddots & \vdots \\
\vdots &  & \ddots & \ddots & \mathbf{K}^\mathrm{T}_{m,m-1} \\
0 & \dots & \dots & 0 & \mathbf{I}_{n_m}
\end{array} \right) 
\end{align}
so $\mathbf{v} = \mathbf{K}^\mathrm{T} \mathbf{x}$ can be expressed as:
\begin{align}
\label{eq:triangle_ad}
\mathbf{v}_i = \mathbf{x}_i + \sum_{j=i+1}^{m} \mathbf{K}^\mathrm{T}_{j,i} \mathbf{x}_j
\end{align}

\section{Partial recursive inverse}
Equation \eqref{eq:triangle} can be transformed to compute $\mathbf{v}$ knowing $\mathbf{x}$ recursively:
\begin{align}
\label{eq:partial_inverse}
\mathbf{v}_i  & = \mathbf{x}_i - \sum_{j=1}^{i-1} \mathbf{K}_{i,j} \mathbf{v}_j
\end{align}
It should be noted that the inverse of $\mathbf{K}_{i,j}$ is not required. This inverse formula is refered to as ``partial'' since the unbalanced variable $\mathbf{v}$ still appears on the right-hand side.\\
$  $\\
For the adjoint in equation \eqref{eq:triangle_ad}, the same transformation leads to:
\begin{align}
\label{eq:partial_inverse_ad}
\mathbf{x}_i = \mathbf{v}_i - \sum_{j=i+1}^{m} \mathbf{K}^\mathrm{T}_{j,i} \mathbf{x}_j
\end{align}
and must be applied recursively in reverse order ($i$ from $m$ to 1).

\section{Full recursive inverse}
We assume that for $1 \le j \le i$, there are matrices $\mathbf{A}_{i, j} \in \mathbb{R}^{n_i \times n_j}$ such that $\mathbf{v}_i$ can be expressed only as a function of $\mathbf{x}$:
\begin{align}
\label{eq:full_inverse}
\mathbf{v}_i = \sum_{j=1}^i \mathbf{A}_{i,j} \mathbf{x}_j
\end{align}
Plugging this expression into the partial recursive inverse formula \eqref{eq:partial_inverse}, we get:
\begin{align}
\mathbf{v}_i & = \mathbf{x}_i - \sum_{j=1}^{i-1} \mathbf{K}_{i,j} \mathbf{v}_j \nonumber \\
& = \mathbf{x}_i - \sum_{j=1}^{i-1} \sum_{k=1}^j \mathbf{K}_{i,j} \mathbf{A}_{j,k} \mathbf{x}_k \nonumber \\
& = \mathbf{x}_i - \sum_{k=1}^{i-1} \sum_{j=k}^{i-1} \mathbf{K}_{i,j} \mathbf{A}_{j,k} \mathbf{x}_k \nonumber \\
& = \mathbf{x}_i - \sum_{j=1}^{i-1} \sum_{k=j}^{i-1} \mathbf{K}_{i,k} \mathbf{A}_{k,j} \mathbf{x}_j \nonumber \\
& =\sum_{j=1}^{i-1} \left(-\sum_{k=j}^{i-1} \mathbf{K}_{i,k} \mathbf{A}_{k,j}\right) \mathbf{x}_j + \mathbf{x}_i
\end{align}
which is consistent with \eqref{eq:full_inverse} if:
\begin{subequations}
\begin{align}
\mathbf{A}_{i,j} & = - \sum_{k=j}^{i-1} \mathbf{K}_{i,k} \mathbf{A}_{k,j} \text{ (for }j < i\text{)}\\
\mathbf{A}_{i,i} & = \mathbf{I}_{n_i}
\end{align}
\end{subequations}
For convenience, we also define the matrices $\mathbf{A}_{i,j}$ for $j > i$ to make the full matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$:
\begin{align}
\mathbf{A}_{i,j} = \mathbf{A}_{j,i}^\mathrm{T}
\end{align}

\section{Sequential estimation}
Since $\mathbf{v}$ is an ``unbalanced variable'', the component $\mathbf{v}_i$ should be uncorrelated with all the other components:
\begin{align}
\label{eq:stat_prop}
\text{if } j \ne i \text{, }\Cov\left(\mathbf{v}_i, \mathbf{v}_j\right) = 0
\end{align}

Using the partial recursive inverse \eqref{eq:partial_inverse} for $j < i$, we get:
\begin{align}
& \Cov\left(\mathbf{v}_i, \mathbf{v}_j\right) = 0 \nonumber \\
\Leftrightarrow \ & \Cov\left(\mathbf{x}_i - \sum_{k=1}^{i-1} \mathbf{K}_{i,k} \mathbf{v}_k, \mathbf{v}_j\right) = 0 \nonumber \\
\Leftrightarrow \ & \Cov\left(\mathbf{x}_i, \mathbf{v}_j\right) - \sum_{k=1}^{i-1} \mathbf{K}_{i,k} \Cov\left(\mathbf{v}_k, \mathbf{v}_j\right) = 0
\end{align}
Using the statistical property \eqref{eq:stat_prop} of $\mathbf{v}$, the term for which $k=j$ is the only one remaining:
\begin{align}
\label{eq:partial_K}
& \Cov\left(\mathbf{x}_i, \mathbf{v}_j\right) - \mathbf{K}_{i,j} \Cov\left(\mathbf{v}_j, \mathbf{v}_j\right) = 0 \nonumber \\
& \mathbf{K}_{i,j} = \Cov\left(\mathbf{x}_i, \mathbf{v}_j\right) \Cov\left(\mathbf{v}_j, \mathbf{v}_j\right)^{-1}
\end{align}

Using the full recursive inverse \eqref{eq:full_inverse}, we get for $j < i$:
\begin{subequations}
\begin{align}
\Cov\left(\mathbf{x}_i,\mathbf{v}_j\right) & = \Cov\left(\mathbf{x}_i,\sum_{k=1}^j \mathbf{A}_{j,k} \mathbf{x}_k\right) \nonumber \\
 & = \sum_{k=1}^j\Cov\left(\mathbf{x}_i,\mathbf{x}_k\right) \mathbf{A}_{j,k}^\mathrm{T} \\
\Cov\left(\mathbf{v}_j,\mathbf{v}_j\right) & = \Cov\left(\sum_{k=1}^j \mathbf{A}_{j,k} \mathbf{x}_k,\sum_{l=1}^j \mathbf{A}_{j,l} \mathbf{x}_l\right) \nonumber \\
 & = \sum_{k=1}^j \mathbf{A}_{j,k} \sum_{l=1}^j \Cov\left(\mathbf{x}_k,\mathbf{x}_l\right) \mathbf{A}_{j,l}^\mathrm{T}
\end{align}
\end{subequations}
which allows a computation of the balance operator $\mathbf{K}_{i,j}$ using expression \eqref{eq:partial_K}.

\section{Aggregated formulation}
For practical reasons explained later in section \ref{sec:practical}, we define aggregated quantities:
\begin{itemize}
\item the aggregated size $\displaystyle \overbar{n}_i = \sum_{j=1}^{i-1} n_j$
\item the aggregated vectors $\overbar{\mathbf{x}}_i$ and $\overbar{\mathbf{v}}_i \in \mathbb{R}^{\overbar{n}_i}$:
\begin{align}
\overbar{\mathbf{x}}_i = \left(\begin{array}{c}
\mathbf{x}_1 \\
\vdots \\
\mathbf{x}_{i-1}
\end{array}
\right) \ \text{ and } \ \overbar{\mathbf{v}}_i = \left(\begin{array}{c}
\mathbf{v}_1 \\
\vdots \\
\mathbf{v}_{i-1}
\end{array}
\right)
\end{align}
\item the aggregated operator $\overbar{\mathbf{K}}_i \in \mathbb{R}^{n_i \times \overbar{n}_i}$:
\begin{align}
\overbar{\mathbf{K}}_i = \left(\mathbf{K}_{i,1} \ \dots \ \mathbf{K}_{i,i-1}\right)
\end{align}
\item the aggregated matrix $\overbar{\mathbf{A}}_i \in \mathbb{R}^{n_i \times \overbar{n}_i}$:
\begin{align}
\overbar{\mathbf{A}}_i = \left(\mathbf{A}_{i,1} \ \dots \ \mathbf{A}_{i,i-1}\right)
\end{align}
\end{itemize}

Thus, equation \eqref{eq:triangle} can be rewritten as:
\begin{align}
\label{eq:triangle_aggregated}
\mathbf{x}_i = \overbar{\mathbf{K}}_i \overbar{\mathbf{v}}_i + \mathbf{v}_i
\end{align}
The partial inverse formulation \eqref{eq:partial_inverse} can be rewritten as:
\begin{align}
\label{eq:partial_inverse_aggregated}
\mathbf{v}_i  & = \mathbf{x}_i - \overbar{\mathbf{K}}_i \overbar{\mathbf{v}}_i
\end{align}
and the full inverse formulation \eqref{eq:full_inverse} can be rewritten as:
\begin{align}
\label{eq:full_inverse_aggregated}
\mathbf{v}_i = \overbar{\mathbf{A}}_i \overbar{\mathbf{x}}_i + \mathbf{x}_i
\end{align}
with $\mathbf{A}_{i,j}$ computed as:
\begin{align}
\mathbf{A}_{i,j} & = - \overbar{\mathbf{K}}_i \overbar{\mathbf{A}}_j^\mathrm{T}
\end{align}
Finally, we define the matrix $\overbar{\overbar{\mathbf{A}}}_i \in \mathbb{R}^{\overbar{n}_i \times \overbar{n}_i}$:
\begin{align}
\overbar{\overbar{\mathbf{A}}}_i = \left(\begin{array}{cccc}
\mathbf{I} & 0 & \dots & 0 \\
\mathbf{A}_{2,1} & \mathbf{I} & \ddots & \vdots \\
\vdots & \ddots & \ddots & 0 \\
\mathbf{A_{i-1,1}} & \dots & \mathbf{A_{i-1,i-2}} & \mathbf{I}
\end{array}\right)
\end{align}
to get the aggregated form:
\begin{align}
\label{eq:aggregated}
\overbar{\mathbf{v}}_i = \overbar{\overbar{\mathbf{A}}}_i \overbar{\mathbf{x}}_i
\end{align}
$  $\\
The assumption \eqref{eq:stat_prop} can be rewritten as:
\begin{align}
\label{eq:stat_prop_aggregated}
\Cov\left(\mathbf{v}_i, \overbar{\mathbf{v}}_i\right) = 0
\end{align}
Expanding eqation \eqref{eq:stat_prop_aggregated} with the partial recursive inverse \eqref{eq:partial_inverse_aggregated} we get:
\begin{align}
\label{eq:partial_K_aggregated}
& \Cov\left(\mathbf{v}_i, \overbar{\mathbf{v}}_i\right) = 0 \nonumber \\
\Leftrightarrow \ & \Cov\left(\mathbf{x}_i - \overbar{\mathbf{K}}_i \overbar{\mathbf{v}}_i, \overbar{\mathbf{v}}_i\right) = 0 \nonumber \\
\Leftrightarrow \ & \Cov\left(\mathbf{x}_i, \overbar{\mathbf{v}}_i\right) - \overbar{\mathbf{K}}_i \Cov\left(\overbar{\mathbf{v}}_i, \overbar{\mathbf{v}}_i\right) = 0 \nonumber \\
\Leftrightarrow \ & \overbar{\mathbf{K}}_i = \Cov\left(\mathbf{x}_i, \overbar{\mathbf{v}}_i\right) \Cov\left(\overbar{\mathbf{v}}_i, \overbar{\mathbf{v}}_i\right)^{-1}
\end{align}
Using the aggregated full recursive inverse \eqref{eq:full_inverse_aggregated} and the aggregated form \eqref{eq:aggregated}, we get:
\begin{subequations}
\begin{align}
\Cov\left(\mathbf{x}_i, \overbar{\mathbf{v}}_i\right) & = \Cov\left(\mathbf{x}_i, \overbar{\overbar{\mathbf{A}}}_i \overbar{\mathbf{x}}_i\right) \nonumber \\
 & = \Cov\left(\mathbf{x}_i, \overbar{\mathbf{x}}_i\right) \overbar{\overbar{\mathbf{A}}}_i^\mathrm{T} \\
\Cov\left(\overbar{\mathbf{v}}_i, \overbar{\mathbf{v}}_i\right) & = \Cov\left(\overbar{\overbar{\mathbf{A}}}_i \overbar{\mathbf{x}}_i, \overbar{\overbar{\mathbf{A}}}_i \overbar{\mathbf{x}}_i\right) \nonumber \\
 & = \overbar{\overbar{\mathbf{A}}}_i \Cov\left(\overbar{\mathbf{x}}_i,  \overbar{\mathbf{x}}_i\right) \overbar{\overbar{\mathbf{A}}}_i^\mathrm{T}
\end{align}
\end{subequations}
which allows a computation of the balance operator $\overbar{\mathbf{K}}_i$ using expression \eqref{eq:partial_K_aggregated}.


\section{Practical computations}
\label{sec:practical}
In practice, covariances are sampled from an ensemble. Let $\left\{\widetilde{\mathbf{x}}^1,\dots,\widetilde{\mathbf{x}}^N\right\}$ be a set of $N$ vectors that samples the distribution of $\mathbf{x}$ and $\left\{\delta \widetilde{\mathbf{x}}^1,\dots,\delta \widetilde{\mathbf{x}}^N\right\}$ their centered counterparts:
\begin{align}
\delta \widetilde{\mathbf{x}}^p = \widetilde{\mathbf{x}}^p - \langle \widetilde{\mathbf{x}} \rangle
\end{align}
where $\langle \cdot \rangle$ denotes the ensemble mean:
\begin{align}
\label{eq:exp_estim}
\langle \widetilde{\mathbf{x}} \rangle = \frac{1}{N} \sum_{p=1}^N \widetilde{\mathbf{x}}^p
\end{align}
The covariance $\Cov \left(\mathbf{x}_i,\mathbf{x}_j\right)$ can be estimated from these perturbations by:
\begin{align}
\label{eq:cov_estim}
\widetilde{\Cov}\left(\mathbf{x}_i,\mathbf{x}_j\right) & = \frac{1}{N-1} \sum_{p=1}^N \delta \widetilde{\mathbf{x}}^p_i \left(\delta \widetilde{\mathbf{x}}^p_j\right)^\mathrm{T}
\end{align}
Because of the limited ensemble size $N$, it can be relevant to filter the covariance matrix with a linear operator $\mathcal{F}$:
\begin{align}
\label{eq:cov_filter}
\overbar{\Cov}\left(\mathbf{x}_i,\mathbf{x}_j\right) = \mathcal{F} \left[\widetilde{\Cov}\left(\mathbf{x}_i,\mathbf{x}_j\right)\right]
\end{align}
$ $\\
The algorithm to compute the balance operator components using the partial recursive inverse equation \eqref{eq:partial_inverse} is detailed in algorithm \ref{algo:partial}. Its counterpart using the full recursive inverse equation \eqref{eq:full_inverse} is detailed in algorithm \ref{algo:full}. It should be noted that:
\begin{itemize}
\item The first method requires storing the ensemble twice, while the second requires storing more matrices.
\item When a filtering $\mathcal{F}$ is applied, both methods can yield different results but there is no theoretical way to know which one is the most accurate.
\end{itemize}

\begin{algorithm}[!ht]
\caption{Recursive computation of the balance operator components using the partial recursive inverse formula \label{algo:partial}}
\begin{algorithmic}
\STATE Copy the ensemble perturbations:
\FOR{$1 \le p \le N$}
\STATE $\delta \widetilde{\mathbf{v}}^p = \delta \widetilde{\mathbf{x}}^p$
\ENDFOR
\STATE $ $
\FOR{$1 \le i \le m$}
\FOR{$1 \le j < i$}
\STATE Estimate the cross-covariance between $\mathbf{x}_i$ and $\mathbf{v}_j$:
\STATE $\displaystyle \qquad \widetilde{\Cov}\left(\mathbf{x}_i,\mathbf{v}_j\right) = \frac{1}{N-1} \sum_{p=1}^N \delta \widetilde{\mathbf{x}}^p_i \left(\delta \widetilde{\mathbf{v}}^p_j\right)^\mathrm{T}$
\STATE Filter the cross-covariance between $\mathbf{x}_i$ and $\mathbf{v}_j$:
\STATE $\displaystyle \qquad \overbar{\Cov}\left(\mathbf{x}_i,\mathbf{v}_j\right) = \mathcal{F} \left[\widetilde{\Cov}\left(\mathbf{x}_i,\mathbf{v}_j\right)\right]$
\STATE Inverse $\overbar{\Cov}\left(\mathbf{v}_j,\mathbf{v}_j\right)$ to get $\overbar{\Cov} \left(\mathbf{v}_j,\mathbf{v}_j\right)^{-1}$
\STATE Compute the balance operator component $\mathbf{K}_{i,j}$:
\STATE $\displaystyle \qquad \mathbf{K}_{i,j} = \overbar{\Cov} \left(\mathbf{x}_i,\mathbf{v}_j\right)\overbar{\Cov} \left(\mathbf{v}_j,\mathbf{v}_j\right)^{-1}$
\STATE $  $\\
\STATE Update the unbalanced ensemble perturbations:
\FOR{$1 \le p \le N$}
\STATE $\displaystyle \qquad \delta \widetilde{\mathbf{v}}^p_i \leftarrow \delta \widetilde{\mathbf{v}}^p_i - \mathbf{K}_{i,j} \delta \widetilde{\mathbf{v}}^p_j$
\ENDFOR
\ENDFOR
\STATE $  $\\
\IF{$i < m$}
\STATE Estimate the auto-covariance of $\mathbf{v}_i$:
\STATE $\displaystyle \qquad \widetilde{\Cov}\left(\mathbf{v}_i,\mathbf{v}_i\right) = \frac{1}{N-1} \sum_{p=1}^N \delta \widetilde{\mathbf{v}}^p_i \left(\delta \widetilde{\mathbf{v}}^p_i\right)^\mathrm{T}$
\STATE Filter the auto-covariance of $\mathbf{v}_i$:
\STATE $\displaystyle \qquad \overbar{\Cov}\left(\mathbf{v}_i,\mathbf{v}_i\right) = \mathcal{F} \left[\widetilde{\Cov}\left(\mathbf{v}_i,\mathbf{v}_i\right)\right]$
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
\caption{Recursive computation of the balance operator components using the full recursive inverse formula \label{algo:full}}
\begin{algorithmic}
\FOR{$1 \le i \le m$}
\FOR{$1 \le j \le i$}
\STATE Estimate the cross-covariance between $\mathbf{x}_i$ and $\mathbf{x}_j$:
\STATE $\displaystyle \qquad \widetilde{\Cov}\left(\mathbf{x}_i,\mathbf{x}_j\right) = \frac{1}{N-1} \sum_{p=1}^N \delta \widetilde{\mathbf{x}}^p_i \left(\delta \widetilde{\mathbf{x}}^p_j\right)^\mathrm{T}$
\STATE Filter the cross-covariance between $\mathbf{x}_i$ and $\mathbf{x}_j$:
\STATE $\displaystyle \qquad \overbar{\Cov}\left(\mathbf{x}_i,\mathbf{x}_j\right) = \mathcal{F} \left[\widetilde{\Cov}\left(\mathbf{x}_i,\mathbf{x}_j\right)\right]$
\ENDFOR
\ENDFOR
\STATE $ $
\FOR{$1 \le i \le m$}
\FOR{$1 \le j < i$}
\STATE Compute the cross-covariance between $\mathbf{x}_i$ and $\mathbf{v}_j$:
\STATE $\displaystyle \qquad \overbar{\Cov}\left(\mathbf{x}_i,\mathbf{v}_j\right) = \sum_{k=1}^{j-1} \overbar{\Cov} \left(\mathbf{x}_i,\mathbf{x}_k\right) \mathbf{A}_{j,k}^\mathrm{T} + \overbar{\Cov} \left(\mathbf{x}_i,\mathbf{x}_j\right)$
\STATE Inverse $\overbar{\Cov}\left(\mathbf{v}_j,\mathbf{v}_j\right)$ to get $\overbar{\Cov} \left(\mathbf{v}_j,\mathbf{v}_j\right)^{-1}$
\STATE Compute the balance operator component $\mathbf{K}_{i,j}$:
\STATE $\displaystyle \qquad \mathbf{K}_{i,j} = \overbar{\Cov} \left(\mathbf{x}_i,\mathbf{v}_j\right)\overbar{\Cov} \left(\mathbf{v}_j,\mathbf{v}_j\right)^{-1}$
\ENDFOR
\STATE $  $\\
\FOR{$1 \le j < i$}
\STATE Compute the matrix $\mathbf{A}_{i,j}$:
\STATE $\displaystyle \qquad \mathbf{A}_{i,j} = - \sum_{k=j}^{i-1} \mathbf{K}_{i,k} \overbar{\mathbf{A}}_{k,j}$
\ENDFOR
\STATE $  $\\
\IF{$i < m$}
\STATE Compute temporary matrices $\mathbf{S}_{k,i}$ :
\FOR{$1 \le k \le i$}
\STATE $\displaystyle \mathbf{S}_{k,i} = \sum_{l=1}^{k} 
 \overbar{\Cov}\left(\mathbf{x}_k,\mathbf{x}_l\right) \mathbf{A}_{i,l}^\mathrm{T} + \sum_{l=k+1}^{i-1} 
 \overbar{\Cov}\left(\mathbf{x}_l,\mathbf{x}_k\right)^\mathrm{T} \mathbf{A}_{i,l}^\mathrm{T} + \overbar{\Cov}\left(\mathbf{x}_k,\mathbf{x}_i\right)$
\ENDFOR
\STATE $ $
\STATE Compute the auto-covariance of $\mathbf{v}_i$:
\STATE $\displaystyle \qquad \overbar{\Cov}\left(\mathbf{v}_i,\mathbf{v}_i\right) = \sum_{k=1}^{i-1} \mathbf{A}_{i,k} \mathbf{S}_{k,i} + \mathbf{S}_{i,i}$
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\end{document}
